{"algorithms":{"#text":"\n","@version":"110505","algorithm":{"#tail":"\n","@name":"ParsCit","#text":"\n","@version":"110505","citationList":{"#tail":"\n","#text":"\n","citation":[{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T1","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T2","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T3","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T4","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T5","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T6","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T7","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T8","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T9","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T10","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T11","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T12","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T13","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T14","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T15","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T16","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T17","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T18","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T19","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T20","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T21","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T22","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T23","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T24","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T25","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T26","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T27","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T28","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T29","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T30","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263-311."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--2"},"marker":{"#tail":"\n","#text":"Brown, Pietra, Pietra, Mercer, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":"ted here as general as possible, I shall assume a totally uninformative concept representation\u2014the trans distribution itself. In other words, I shall assume that each different pair of word sequence types is deterministically generated from a different concept, so that trans (Ili, ViIC) is zero for all concepts except one. Now, a bag-to-bag translation model can be fully specified by the distributions of 1 and trans. Pr (Bi, A, B21/, trans) = Pr(/) \u2022 1! H trans(ii,Ari) (11) (i\u2022/) EA The probability distribution trans (ii, it') is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b), this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other half, so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8. 3. The One-to-One Assumption The most general word-to-word translation model trans(ii, V), where ii and range over sequences in Li and £2, has an i","@endWordPosition":"2180","@position":"13893","annotationId":"T31","@startWordPosition":"2177","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"slations. (Wu and Xia 1994, 211) This is a reasonable evaluation method, but it is not comparable to methods that simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and Lange 1994; Melamed 1996b). A weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate. Therefore, weighted precision estimates are generally higher than unweighted ones. 4.3 Reestimated Sequence-to-Sequence Translation Models Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by Brown et al. (1993b). These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution. I shall review these models using the notation in Table 1. 228 Melamed Models of Translational Equivalence 4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues employ the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the parameters of their Model 1. On iteration i, the EM algorithm reestimates the model parameters transi(vju) based on their estimates from iteration i","@endWordPosition":"3825","@position":"23947","annotationId":"T32","@startWordPosition":"3822","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"ther than merely approximated. 231 Computational Linguistics Volume 26, Number 2 The number of possible assignments grows exponentially with the size of aligned text segments in the bitext. Due to the parameter interdependencies introduced by the one-to-one assumption, we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in Brown et al. [1993b, Equation 26]). Barring such a decomposition method, the MLE approach is infeasible. This is why we must make do with approximations to the EM algorithm. In this situation, Brown et al. (1993b, 293) recommend &quot;evaluating the expectations using only a single, probable alignment.&quot; The single most probable assignment Amax is the maximum a posteriori (MAP) assignment: Amax \u2014 arglinea!4(Pr(U,A, Vie) arg max Pr(1) \u2022 1! 11 trans(ui, V]) AEA (i,j)EA [ arg imic log Pr(1) \u2022 1! IT trans(ui,Vi) (i,j)EA arg max AEA log[Pr(1) \u2022 1!] + log trans(ui,vi) (i,DEA To simplify things further, let us assume that Pr(/) \u2022 1! is constant, so that Amax = arg max log trans(ui,vi). AEA (i,i)EA If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), then the right-hand side of ","@endWordPosition":"5411","@position":"33714","annotationId":"T33","@startWordPosition":"5408","@citStr":"Brown et al. (1993"},{"#tail":"\n","#text":"are translated less consistently than rare words (Catizone, Russell, and Warwick 1989). To account for these differences, we can estimate separate values of A+ and A- for different ranges of cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts of speech. A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts. Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not (cf. Brown et al. 1993). When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B(links (u, v) Icooc(u, v), A-zF ) (37) scorec (u, viz = class (u, v)) = log B (links (u, v)lcooc(u, v), ) Section 6.1.1 describes the link classes used in the experiments below. 6. Evaluation 6.1 Evaluation at the Token Level This section compares translation model estimation methods A, B, and C to each other and to Brown et al.'s (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence information only; Method A is based on the one-to-one assumption;","@endWordPosition":"8296","@position":"50682","annotationId":"T34","@startWordPosition":"8293","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"bols, such as - and * NU the NULL word, in a class by itself C Content words: nouns, adjectives, adverbs, non-auxiliary verbs F all other words, i.e., function words method is the list of function words in class F. Certainly, more sophisticated word classification methods could produce better models, but even the simple classification in Table 4 should suffice to demonstrate the method's potential. 6.1.1 Experiment 1. Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b). Objective and more accurate tests can be carried out using a &quot;gold standard.&quot; I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English. This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details). The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool. The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular t","@endWordPosition":"8678","@position":"53058","annotationId":"T35","@startWordPosition":"8675","@citStr":"Brown et al. 1993"},{"#tail":"\n","#text":"and I. Figure 8 graphs the accuracy of the method against coverage, with 95% confidence intervals. The upper curve represents accuracy when incomplete links are considered correct, and the lower when they are considered incorrect. On the former metric, the method can generate translation lexicons with accuracy and coverage both exceeding 90%, as well as dictionary-size translation lexicons that are over 99% correct. 7. Conclusion There are many ways to model translational equivalence and many ways to estimate translation models. &quot;The mathematics of statistical machine translation&quot; proposed by Brown et al. (1993b) are just one kind of mathematics for one kind of statistical transFigure 8 Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. (99.2%) 99.0%) incomplete = correct 100 98 96 _ - >, o94 - LT_ = 0 92 0 - (91.6%) as _ 88 86 .1(89:2%) ------______ incomplete = incorrect (92.8%) (86.8%) ..,1,:. 90 0 i i 36 46 (3/0 coverage 84 90 246 Melamed Models of Translational Equivalence lation. In this article, I have proposed and evaluated new kinds of translation model biases, alternative parameter estimation strategies, and techniques for exploiting preexisting knowl","@endWordPosition":"11420","@position":"70069","annotationId":"T36","@startWordPosition":"11417","@citStr":"Brown et al. (1993"}]},"title":{"#tail":"\n","#text":"The mathematics of statistical machine translation: Parameter estimation."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Peter F Brown"},{"#tail":"\n","#text":"Stephen A Della Pietra"},{"#tail":"\n","#text":"Vincent J Della Pietra"},{"#tail":"\n","#text":"Robert L Mercer"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1998"},"rawString":{"#tail":"\n","#text":"Candito, Marie-Helene. 1998. Building parallel LTAG for French and Italian. In COLING-ACL '98: 36 Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 211-217, Montreal, Canada."},"#text":"\n","pages":{"#tail":"\n","#text":"211--217"},"marker":{"#tail":"\n","#text":"Candito, 1998"},"location":{"#tail":"\n","#text":"Montreal, Canada."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"of Translational Equivalence each i is distinct, and each j is distinct. The label pairs in a given assignment can be generated in any order, so there are 1! ways to generate an assignment of size 1.6 It follows that the probability of generating a pair of bags (B1, B2) with a particular assignment A of size 1 is Pr(Bi, A, B21/, C , trans) = Pr (1) 1! ri E Pr(C)trans(tii, Ari1C). (10) CEC The above equation holds regardless of how we represent concepts. There are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars (Abeille et al. 1990; Shieber 1994; Candito 1998), lexical conceptual structures (Dorr 1992) and WordNet synsets (Fellbaum 1998; Vossen 1998). Of course, for a representation to be used, a method must exist for estimating its distribution in data. A useful representation will reduce the entropy of the trans distribution, which is conditioned on the concept distribution as shown in Equation 10. This topic is beyond the scope of this article, however. I mention it only to show how the models presented here may be used as building blocks for models that are more psycholinguistically sophisticated. To make the translation model estimation method","@endWordPosition":"1983","@position":"12667","annotationId":"T37","@startWordPosition":"1982","@citStr":"Candito 1998"}},"title":{"#tail":"\n","#text":"Building parallel LTAG for French and Italian."},"booktitle":{"#tail":"\n","#text":"In COLING-ACL '98: 36 Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Marie-Helene Candito"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1993"},"rawString":{"#tail":"\n","#text":"Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19(1):61-74."},"journal":{"#tail":"\n","#text":"Computational Linguistics"},"#text":"\n","pages":{"#tail":"\n","#text":"19--1"},"marker":{"#tail":"\n","#text":"Dunning, 1993"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"ustly by considering the whole table. For example, Gale and Church (1991, 154) suggest that &quot;02, a x2-like statistic, seems to be a particularly good choice because it makes good use of the off-diagonal cells&quot; in the contingency table. 10 At least for my current very inefficient implementation. (24) 232 Melamed Models of Translational Equivalence Table 2 A co-occurrence contingency table. Total cooc(., v) cooc(., \u2014.v) cooc(., .) cooc(u,.v) cooc(-11, v) cooc(u, \u2014.v) COOC(\u2014V, Total cooc(-u, -) In informal experiments described elsewhere (Melamed 1995), I found that the G2 statistic suggested by Dunning (1993) slightly outperforms 02. Let the cells of the contingency table be named as follows: a Now, G2(u, v) = \u201421og B(ala + b,pi)B(cic + d,p2) (27) B(aia + b,p)B(cic + d,p) nk where B(kin,p) = () pk(1 p)n-k are binomial probabilities. The statistic uses maximum likelihood estimates for the probability parameters: p1 = bf P2 \u2014 c±cd' P = a±ab±±cc-Fd' G2 is easy to compute because the binomial coefficients in the numerator and in the denominator cancel each other out. All my methods initialize the parameters score(u,v) to G2(u, v), except that any pairing with NULL is initialized to an infinitesimal va","@endWordPosition":"5762","@position":"35896","annotationId":"T38","@startWordPosition":"5761","@citStr":"Dunning (1993)"}},"title":{"#tail":"\n","#text":"Accurate methods for the statistics of surprise and coincidence."},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"Ted Dunning"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1999"},"institution":{"#tail":"\n","#text":"College Park, MD. Association for Computational Linguistics."},"rawString":{"#tail":"\n","#text":"McCarley, J. Scott. 1999. Should we translate the documents or the queries in cross-language information retrieval? In Proceedings of the 37th Annual Meeting, pages 208-214, College Park, MD. Association for Computational Linguistics."},"#text":"\n","pages":{"#tail":"\n","#text":"208--214"},"marker":{"#tail":"\n","#text":"McCarley, 1999"},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":" to denote collections, including sequences and bags, and italics for scalar variables. I shall also distinguish between types and tokens by using bold font for the former and plain font for the latter. 2. Translation Model Decomposition There are two kinds of applications of translation models: those where word order plays a crucial role and those where it doesn't. Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications. Applications where word order is not essential include \u2022 cross-language information retrieval (e.g., McCarley 1999), \u2022 multilingual document filtering (e.g., Oard 1997), \u2022 computer-assisted language learning (e.g., Nerbonne et al. 1997), \u2022 certain machine-assisted translation tools (e.g., Macklovitch 1994; Melamed 1996a), \u2022 concordancing for bilingual lexicography (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991), 222 Melamed Models of Translational Equivalence \u2022 corpus linguistics (e.g., Svartvik 1992), \u2022 &quot;crummy&quot; machine translation (e.g., Church and Hovy 1992; Resnik 1997). For these applications, empirically estimated models have a number of advantages over handcrafted models such as on","@endWordPosition":"836","@position":"5628","annotationId":"T39","@startWordPosition":"835","@citStr":"McCarley 1999"}},"title":{"#tail":"\n","#text":"Should we translate the documents or the queries in cross-language information retrieval?"},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 37th Annual Meeting,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"J Scott McCarley"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1995"},"rawString":{"#tail":"\n","#text":"Melamed, I. Dan. 1995. Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons. In Proceedings of the Third Workshop on Very Large Corpora, pages 184-198, Cambridge, MA."},"#text":"\n","pages":{"#tail":"\n","#text":"184--198"},"marker":{"#tail":"\n","#text":"Melamed, 1995"},"location":{"#tail":"\n","#text":"Cambridge, MA."},"contexts":{"#tail":"\n","#text":"\n","context":[{"#tail":"\n","#text":" 5. 226 Melamed Models of Translational Equivalence He nods his head II I hoche la tete Figure 1 nods and hoche often co-occur, as do nods and head. The direct association between nods and hoche, and the direct association between nods and head give rise to an indirect association between hoche and head. 4.2 Nonprobabilistic Translation Lexicons Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; Melamed 1995; Wu and Xia 1994). Most of these algorithms can be summarized as follows: 1. Choose a similarity function S between word types in Li and word types in L. 2. Compute association scores S(u, v) for a set of word type pairs (u, v) E (Li x £2) that occur in training data. 3. Sort the word pairs in descending order of their association scores. 4. Discard all word pairs for which S(u, v) is less than a chosen threshold. The remaining word pairs become the entries in the translation lexicon. The various proposals differ mainly in their choice of similarity function. Almost all the similarity functio","@endWordPosition":"2996","@position":"18946","annotationId":"T40","@startWordPosition":"2995","@citStr":"Melamed 1995"},{"#tail":"\n","#text":"ependence between two word types can be estimated more robustly by considering the whole table. For example, Gale and Church (1991, 154) suggest that &quot;02, a x2-like statistic, seems to be a particularly good choice because it makes good use of the off-diagonal cells&quot; in the contingency table. 10 At least for my current very inefficient implementation. (24) 232 Melamed Models of Translational Equivalence Table 2 A co-occurrence contingency table. Total cooc(., v) cooc(., \u2014.v) cooc(., .) cooc(u,.v) cooc(-11, v) cooc(u, \u2014.v) COOC(\u2014V, Total cooc(-u, -) In informal experiments described elsewhere (Melamed 1995), I found that the G2 statistic suggested by Dunning (1993) slightly outperforms 02. Let the cells of the contingency table be named as follows: a Now, G2(u, v) = \u201421og B(ala + b,pi)B(cic + d,p2) (27) B(aia + b,p)B(cic + d,p) nk where B(kin,p) = () pk(1 p)n-k are binomial probabilities. The statistic uses maximum likelihood estimates for the probability parameters: p1 = bf P2 \u2014 c±cd' P = a±ab±±cc-Fd' G2 is easy to compute because the binomial coefficients in the numerator and in the denominator cancel each other out. All my methods initialize the parameters score(u,v) to G2(u, v), except that ","@endWordPosition":"5752","@position":"35837","annotationId":"T41","@startWordPosition":"5751","@citStr":"Melamed 1995"}]},"title":{"#tail":"\n","#text":"Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the Third Workshop on Very Large Corpora,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":{"#tail":"\n","#text":"I Dan Melamed"}}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Nerbonne, John, Lauri Karttunen, Elena Paskaleva, Gabor Proszeky, and Tiit Roosmaa. 1997. Reading more into foreign languages. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 135-138, Washington, DC."},"#text":"\n","pages":{"#tail":"\n","#text":"135--138"},"marker":{"#tail":"\n","#text":"Nerbonne, Karttunen, Paskaleva, Proszeky, Roosmaa, 1997"},"location":{"#tail":"\n","#text":"Washington, DC."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"etween types and tokens by using bold font for the former and plain font for the latter. 2. Translation Model Decomposition There are two kinds of applications of translation models: those where word order plays a crucial role and those where it doesn't. Empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications. Applications where word order is not essential include \u2022 cross-language information retrieval (e.g., McCarley 1999), \u2022 multilingual document filtering (e.g., Oard 1997), \u2022 computer-assisted language learning (e.g., Nerbonne et al. 1997), \u2022 certain machine-assisted translation tools (e.g., Macklovitch 1994; Melamed 1996a), \u2022 concordancing for bilingual lexicography (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991), 222 Melamed Models of Translational Equivalence \u2022 corpus linguistics (e.g., Svartvik 1992), \u2022 &quot;crummy&quot; machine translation (e.g., Church and Hovy 1992; Resnik 1997). For these applications, empirically estimated models have a number of advantages over handcrafted models such as on-line versions of bilingual dictionaries. Two of the advantages are the possibility of better coverage and the possibilit","@endWordPosition":"852","@position":"5749","annotationId":"T42","@startWordPosition":"849","@citStr":"Nerbonne et al. 1997"}},"title":{"#tail":"\n","#text":"Reading more into foreign languages."},"booktitle":{"#tail":"\n","#text":"In Proceedings of the 5th ACL Conference on Applied Natural Language Processing,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"John Nerbonne"},{"#tail":"\n","#text":"Lauri Karttunen"},{"#tail":"\n","#text":"Elena Paskaleva"},{"#tail":"\n","#text":"Gabor Proszeky"},{"#tail":"\n","#text":"Tiit Roosmaa"}]}},{"#tail":"\n","date":{"#tail":"\n","#text":"1997"},"rawString":{"#tail":"\n","#text":"Resnik, Philip, and David Yarowsky. 1997. A perspective on word sense disambiguation methods and their evaluation. hi Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 79-86, Washington, DC."},"#text":"\n","pages":{"#tail":"\n","#text":"79--86"},"marker":{"#tail":"\n","#text":"Resnik, Yarowsky, 1997"},"location":{"#tail":"\n","#text":"Washington, DC."},"contexts":{"#tail":"\n","#text":"\n","context":{"#tail":"\n","#text":"be translated as peine than as phrase. &quot;In the vicinity of&quot; is one kind of collocation. Co-occurrence 11 The competitive linking algorithm can be generalized to stop searching before the number of possible assignments is reduced to one, at which point the link counts can be computed as probabilistically weighted averages over the remaining assignments. I use this method to resolve ties. 234 links(u,v) / cooc(u, v) Figure 2 The ratio links(u, v) I cooc (u, v), for several values of cooc(u, v). in bitext space is another kind of collocation. If each word's translation is treated as a sense tag (Resnik and Yarowsky 1997), then &quot;translational&quot; collocations have the unique property that the collocate and the word sense are one and the same! Method B exploits this property under the hypothesis that &quot;one sense per collocation&quot; holds for translational collocations. This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be linked. To test this hypothesis, I ran one iteration of Method A on 300,000 aligned sentence pairs from the Canadian Hansards bit","@endWordPosition":"6753","@position":"41973","annotationId":"T43","@startWordPosition":"6750","@citStr":"Resnik and Yarowsky 1997"}},"title":{"#tail":"\n","#text":"A perspective on word sense disambiguation methods and their evaluation."},"booktitle":{"#tail":"\n","#text":"hi Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics,"},"@valid":"true","authors":{"#tail":"\n","#text":"\n","author":[{"#tail":"\n","#text":"Philip Resnik"},{"#tail":"\n","#text":"David Yarowsky"}]}}]}}}}
