<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000782">
<title confidence="0.998095">
Processes that Shape Conversation
and their Implications for Computational Linguistics
</title>
<author confidence="0.995926">
Susan E. Brennan
</author>
<affiliation confidence="0.9984385">
Department of Psychology
State University of New York
</affiliation>
<address confidence="0.707881">
Stony Brook, NY, US 11794-2500
</address>
<email confidence="0.994435">
susan.brennan@sunysb.edu
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917">
Experimental studies of interactive language use
have shed light on the cognitive and
interpersonal processes that shape conversation;
corpora are the emergent products of these
processes. I will survey studies that focus on
under-modelled aspects of interactive language
use, including the processing of spontaneous
speech and disfluencies; metalinguistic displays
such as hedges; interactive processes that affect
choices of referring expressions; and how
communication media shape conversations. The
findings suggest some agendas for
computational linguistics.
</bodyText>
<sectionHeader confidence="0.961462" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999960708333334">
Language is shaped not only by
grammar, but also by the cognitive processing of
speakers and addressees, and by the medium in
which it is used. These forces have, until
recently, received little attention, having been
originally consigned to &amp;quot;performance&amp;quot; by
Chomsky, and considered to be of secondary
importance by many others. But as anyone who
has listened to a tape of herself lecturing surely
knows, spoken language is formally quite
different from written language. And as those
who have transcribed conversation are
excruciatingly aware, interactive, spontaneous
speech is especially messy and disfluent. This
fact is rarely acknowledged by psychological
theories of comprehension and production
(although see Brennan &amp; Schober, in press;
Clark, 1994, 1997; Fox Tree, 1995). In fact,
experimental psycholinguists still make up most
of their materials, so that much of what we know
about sentence processing is based on a
sanitized, ideal form of language that no one
actually speaks.
But the field of computational
linguistics has taken an interesting turn:
Linguists and computational linguists who
formerly used made-up sentences are now using
naturally- and experimentally-generated corpora
on which to base and test their theories. One of
the most exciting developments since the early
1990s has been the focus on corpus data.
Organized efforts such as LDC and ELRA have
assembled large and varied corpora of speech
and text, making them widely available to
researchers and creators of natural language and
speech recognition systems. Finally, Internet
usage has generated huge corpora of interactive
spontaneous text or &amp;quot;visible conversations&amp;quot; that
little resemble edited texts.
Of course, ethnographers and
sociolinguists who practice conversation
analysis (e.g., Sacks, Schegloff, &amp; Jefferson,
1974; Goodwin, 1981) have known for a long
time that spontaneous interaction is interesting
in its own right, and that although conversation
seems messy at first glance, it is actually
orderly. Conversation analysts have
demonstrated that speakers coordinate with each
other such feats as achieving a joint focus of
attention, producing closely timed turn
exchanges, and finishing each another’s
utterances. These demonstrations have been
compelling enough to inspire researchers from
psychology, linguistics, computer science, and
human-computer interaction to turn their
attention to naturalistic language data.
But it is important to keep in mind that a
corpus is, after all, only an artifact—a product
that emerges from the processes that occur
between and within speakers and addressees.
Researchers who analyze the textual records of
conversation are only overhearers, and there is
ample evidence that overhearers experience a
conversation quite differently from addressees
and from side participants (Schober &amp; Clark,
1989; Wilkes-Gibbs &amp; Clark, 1992). With a
corpus alone, there is no independent evidence
of what people actually intend or understand at
different points in a conversation, or why they
make the choices they do. Conversation
experiments that provide partners with a task to
do have much to offer, such as independent
measures of communicative success as well as
evidence of precisely when one partner is
confused or has reached a hypothesis about the
other’s beliefs or intentions. Task-oriented
corpora in combination with information about
how they were generated are important for
discourse studies.
We still don&apos;t know nearly enough about
the cognitive and interpersonal processes that
underlie spontaneous language use—how
speaking and listening are coordinated between
individuals as well as within the mind of
someone who is switching speaking and
listening roles in rapid succession. Hence,
determining what information needs to be
represented moment by moment in a dialog
model, as well as how and when it should be
updated and used, is still an open frontier. In this
paper I start with an example and identify some
distinctive features of spoken language
interchanges. Then I describe several
experiments aimed at understanding the
processes that generate them. I conclude by
proposing some desiderata for a dialog model.
</bodyText>
<subsectionHeader confidence="0.820637">
Two people in search of a perspective
</subsectionHeader>
<bodyText confidence="0.9954465">
To begin, consider the following
conversational interchange from a laboratory
experiment on referential communication. A
director and a matcher who could not see each
another were trying to get identical sets of
picture cards lined up in the same order.
</bodyText>
<listItem confidence="0.764098">
(1) D:ah boy this one ah boy
all right it looks kinda like-
</listItem>
<bodyText confidence="0.904660622222222">
on the right top there’s a square that looks
diagonal
M: uh huh
D: and you have sort of another like rectangle
shape, the-
like a triangle, angled, and on the bottom it’s uh
I don’t know what that is, glass shaped
M: all right I think I got it
D: it’s almost like a person kind of in a weird way
M: yeah like like a monk praying or something
D: right yeah good great
M: all right I got it
(Stellmann &amp; Brennan, 1993)
Several things are apparent from this exchange.
First, it contains several disfluencies or
interruptions in fluent speech. The director
restarts her first turn twice and her second turn
once. She delivers a description in a series of
installments, with backchannels from the
matcher to confirm them. She seasons her
speech with fillers like uh, pauses occasionally,
and displays her commitment (or lack thereof) to
what she is saying with displays like ah boy this
one ah boy and I don’t know what that is. Even
though she is the one who knows what the target
picture is, it is the matcher who ends up
proposing the description that they both end up
ratifying: like a monk praying or something.
Once the director has ratified this proposal, they
have succeeded in establishing a conceptual pact
(see Brennan &amp; Clark, 1996). En route, both
partners hedged their descriptions liberally,
marking them as provisional, pending evidence
of acceptance from the other. This example is
typical; in fact, 24 pairs of partners who
discussed this object ended up synthesizing
nearly 24 different but mutually agreed-upon
perspectives. Finally, the disfluencies, hedges,
and turns would have been distributed quite
differently if this conversation had been
conducted over a different medium—through
instant messaging, or if the partners had had
visual contact. Next I will consider the proceses
that underlie these aspects of interactive spoken
communication.
</bodyText>
<sectionHeader confidence="0.726578" genericHeader="method">
1 Speech is disfluent, and disfluencies
</sectionHeader>
<subsectionHeader confidence="0.738933">
bear information
</subsectionHeader>
<bodyText confidence="0.999976508064516">
The implicit assumptions of
psychological and computational theories that
ignore disfluencies must be either that people
aren&apos;t disfluent, or that disfluencies make
processing more difficult, and so theories of
fluent speech processing should be developed
before the research agenda turns to disfluent
speech processing. The first assumption is
clearly false; disfluency rates in spontaneous
speech are estimated by Fox Tree (1995) and by
Bortfeld, Leon, Bloom, Schober, and Brennan
(2000) to be about 6 disfluencies per 100 words,
not including silent pauses. The rate is lower for
speech to machines (Oviatt, 1995; Shriberg,
1996), due in part to utterance length; that is,
disfluency rates are higher in longer utterances,
where planning is more difficult, and utterances
addressed to machines tend to be shorter than
those addressed to people, often because
dialogue interfaces are designed to take on more
initiative. The average speaker may believe,
quite rightly, that machines are imperfect speech
processors, and plan their utterances to machines
more carefully. The good news is that speakers
can adapt to machines; the bad news is that they
do so by recruiting limited cognitive resources
that could otherwise be focused on the task
itself. As for the second assumption, if the goal
is to eventually process unrestricted, natural
human speech, then committing to an early and
exclusive focus on processing fluent utterances
is risky. In humans, speech production and
speech processing are done incrementally, using
contextual information from the earliest
moments of processing (see, e.g., Tanenhaus et
al. 1995). This sort of processing requires quite a
different architecture and different mechanisms
for ambiguity resolution than one that begins
processing only at the end of a complete and
well-formed utterance. Few approaches to
parsing have tried to handle disfluent utterances
(notable exceptions are Core &amp; Schubert, 1999;
Hindle, 1983; Nakatani &amp; Hirschberg, 1994;
Shriberg, Bear, &amp; Dowding, 1992).
The few psycholinguistic experiments
that have examined human processing of
disfluent speech also throw into question the
assumption that disfluent speech is harder to
process than fluent speech. Lickley and Bard
(1996) found evidence that listeners may be
relatively deaf to the words in a reparandum (the
part that would need to be excised in order for
the utterance to be fluent), and Shriberg and
Lickley (1993) found that fillers such as um or
uh may be produced with a distinctive intonation
that helps listeners distinguish them from the
rest of the utterance. Fox Tree (1995) found that
while previous restarts in an utterance may slow
a listener’s monitoring for a particular word,
repetitions don’t seem to hurt, and some fillers,
such as uh, seem to actually speed monitoring
for a subsequent word.
What information exists in disfluencies,
and how might speakers use it? Speech
production processes can be broken into three
phases: a message or semantic process, a
formulation process in which a syntactic frame
is chosen and words are filled in, and an
articulation process (Bock, 1986; Bock &amp;
Levelt, 1994; Levelt, 1989). Speakers monitor
their speech both internally and externally; that
is, they can make covert repairs at the point
when an internal monitoring loop checks the
output of the formulation phase before
articulation begins, or overt repairs when a
problem is discovered after the articulation
phase via the speaker&apos;s external monitor—the
point at which listeners also have access to the
signal (Levelt, 1989). According to
Nooteboom&apos;s (1980) Main Interruption Rule,
speakers tend to halt speaking as soon as they
detect a problem. Production data from Levelt&apos;s
(1983) corpus supported this rule; speakers
interrupted themselves within or right after a
problem word 69% of the time.
How are regularities in disfluencies
exploited by listeners? We have looked at the
comprehension of simple fluent and disfluent
instructions in a constrained situation where the
listener had the opportunity to develop
expectations about what the speaker would say
(Brennan &amp; Schober, in press). We tested two
hypotheses drawn from some suggestions of
Levelt&apos;s (1989): that &amp;quot;by interrupting a word, a
speaker signals to the addressee that the word is
an error,&amp;quot; and that an editing expression like er
or uh may &amp;quot;warn the addressee that the current
message is to be replaced,&amp;quot; as with Move to the
ye— uh, orange square. We collected naturally
fluent and disfluent utterances by having a
speaker watch a display of objects; when one
was highlighted he issued a command about it,
like &amp;quot;move to the yellow square.&amp;quot; Sometimes the
highlight changed suddenly; this sometimes
caused the speaker to produce disfluencies. We
recorded enough tokens of simple disfluencies to
compare the impact of three ways in which
speakers interrupt themselves: immediately after
a problem word, within a problem word, or
within a problem word and with the filler uh.
We reasoned that if a disfluency indeed
bears useful information, then we should be able
to find a situation where a target word is faster
to comprehend in a disfluent utterance than in a
fluent one. Imagine a situation in which a
listener expects a speaker to refer to one of two
objects. If the speaker begins to name one and
then stops and names the other, the way in
which she interrupts the utterance might be an
early clue as to her intentions. So the listener
may be faster to recognize her intentions relative
to a target word in a disfluent utterance than in
an utterance in which disfluencies are absent.
We compared the following types of utterances:
</bodyText>
<listItem confidence="0.990883">
a. Move to the orange square (naturally fluent)
b. Move to the jorange square (disfluency excised)
c. Move to the yellow- orange square
d. Move to the ye- orange square
e. Move to the ye- uh, orange square
f. Move to the orange square
g. Move to the ye- orange square
h. Move to the uh, orange square
</listItem>
<bodyText confidence="0.999818545454545">
Utterances c, d, and e were spontaneous
disfluencies, and f, g, and h were edited versions
that replaced the removed material with pauses
of equal length to control for timing. In
utterances c—h, the reparandum began after the
word the and continued until the interruption
site (after the unintended color word, color word
fragment, or location where this information had
been edited out). The edit interval in c—h began
with the interruption site, included silence or a
filler, and ended with the onset of the repair
color word. Response times were calculated
relative to the onset of the repair, orange.
The results were that listeners made
fewer errors, the less incorrect information they
heard in the reparandum (that is, the shorter the
reparandum), and they were faster to respond to
the target word when the edit interval before the
repair was longer. They comprehended target
words after mid-word interruptions with fillers
faster than they did after mid-word interruptions
without fillers (since a filler makes the edit
interval longer), and faster than they did when
the disfluency was replaced by a pause of equal
length. This filler advantage did not occur at the
expense of accuracy—unlike with disfluent
utterances without fillers, listeners made no
more errors on disfluent utterances with fillers
than they did on fluent utterances. These
findings highlight the importance of timing in
speech recognition and utterance interpretation.
The form and length of the reparandum and edit
interval bear consequences for how quickly a
disfluent utterance is processed as well as for
whether the listener makes a commitment to an
interpretation the speaker does not intend.
Listeners respond to pauses and fillers
on other levels as well, such as to make
inferences about speakers’ alignment to their
utterances. People coordinate both the content
and the process of conversation; fillers, pauses,
and self-speech can serve as displays by
speakers that provide an account to listeners for
difficulties or delays in speaking (Clark, 1994;
Clark, 1997; Clark &amp; Brennan, 1991). Speakers
signal their Feeling-of-Knowing (FOK) when
answering a question by the displays they put on
right before the answer (or right before they
respond with I don’t know) (Brennan &amp;
Williams, 1995; Smith &amp; Clark, 1993). In these
experiments, longer latencies, especially ones
that contained fillers, were associated with
answers produced with a lower FOK and that
turned out to be incorrect. Thus in the following
example, A1 displayed a lower FOK than A2:
</bodyText>
<listItem confidence="0.558491333333333">
Q: Who founded the American Red Cross?
A1: um Florence Nightingale?
A2: Clara Barton.
</listItem>
<bodyText confidence="0.993898264705882">
Likewise, non-answers (e.g., I don’t know) after
a filler or a long latency were produced by
speakers who were more likely to recognize the
correct answers later on a multiple choice test;
those who produced a non-answer immediately
did not know the answers. Not only do speakers
display their difficulties and metalinguistic
knowledge using such devices, but listeners can
process this information to produce an accurate
Feeling-of-Another&apos;s-Knowing, or estimate of
the speaker’s likelihood of knowing the correct
answer (Brennan &amp; Williams, 1995).
These programs of experiments hold
implications for both the generation and
interpretation of spoken utterances. A system
could indicate its confidence in its message with
silent pauses, fillers, and intonation, and users
should be able to interpret this information
accurately. If machine speech recognition were
conducted in a fashion more like human speech
recognition, timing would be a critical cue and
incremental parses would be continually made
and unmade. Although this approach would be
computationally expensive, it might produce
better results with spontaneous speech.
2 Referring expressions are provisional
until ratified by addressees.
Consider again the exchange in Example
(1). After some work, the director and matcher
eventually settled on a mutual perspective.
When they finished matching the set of 12
picture cards, the cards were shuffled and the
task was repeated several more times. In the
very next round, the conversation went like this:
</bodyText>
<listItem confidence="0.647798">
(2) B: nine is that monk praying
A: yup
Later on, referring was even more efficient:
(3) A: three is the monk
B: ok
</listItem>
<bodyText confidence="0.999966530303031">
A and B, who switched roles on each round,
marked the fact that they had achieved a mutual
perspective by reusing the same term, monk, in
repeated references to the same object. These
references tend to shorten over time. In Brennan
and Clark (1996), we showed that once people
coordinate a perspective on an object, they tend
to continue to use the same terms that mark that
shared perspective (e.g., the man’s pennyloafer),
even when they could use an even shorter basic-
level term (e.g., the shoe, when the set of objects
has changed such that it no longer needs to be
distinguished from other shoes in the set). This
process of conceptual entrainment appears to be
partner-specific—upon repeated referring to the
same object but with a new partner, speakers
were more likely to revert to the basic level
term, due in part to the feedback they received
from their partners (Brennan &amp; Clark, 1996).
These examples depict the interpersonal
processes that lead to conceptual entrainment.
The director and matcher used many hedges in
their initial proposals and counter-proposals
(e.g., it’s almost like a person kind of in a weird
way, and yeah like like a monk praying or
something). Hedges dropped out upon repeated
referring. We have proposed (Brennan &amp; Clark,
1996) that hedges are devices for signaling a
speaker&apos;s commitment to the perspective she is
proposing. Hedges serve social needs as well, by
inviting counter-proposals from the addressee
without risking loss of face due to overt
disagreements (Brennan &amp; Ohaeri, 1999).
It is worth noting that people&apos;s referring
expressions converge not only with those of
their human partners, but also with those of
computer partners (Brennan, 1996; Ohaeri,
1995). In our text and spoken dialogue Wizard-
of-Oz studies, when simulated computer
partners used deliberately different terms than
the ones people first presented to them, people
tended to adopt the computers&apos; terms, even
though the computers had apparently
&amp;quot;understood&amp;quot; the terms people had first
produced (Brennan, 1996; Ohaeri, 1995).
The impetus toward conceptual
entrainment marked by repeated referring
expressions appears to be so compelling that
native speakers of English will even produce
non-idiomatic referring expressions (e.g., the
chair in which I shake my body, referring to a
rocking chair) in order to ratify a mutually-
achieved perspective with non-native speakers
(Bortfeld &amp; Brennan, 1987).
Such findings hold many implications
for utterance generation and the design of
dialogue models. Spoken and text dialogue
interfaces of the future should include resources
for collaboration, including those for negotiating
meanings, modeling context, recognizing which
referring expressions are likely to index a
particular conceptualization, keeping track of the
referring expressions used by a partner so far,
and reusing those expressions. This would help
solve the “vocabulary problem” in human-
computer interaction (Brennan, to appear).
</bodyText>
<sectionHeader confidence="0.870235" genericHeader="method">
3 Grounding varies with the medium
</sectionHeader>
<bodyText confidence="0.999600333333333">
Grounding is the process by which people
coordinate their conversational activities,
establishing, for instance, that they understand
one another well enough for current purposes.
There are many activities to coordinate in
conversation, each with its own cost, including:
</bodyText>
<listItem confidence="0.999699285714286">
• getting an addressee’s attention in order to begin
the conversation
• planning utterances the addressee is likely to
understand
• producing utterances
• recognizing when the addressee does not
understand
• initiating and managing repairs
• determining what inferences to make when there
is a delay
• receiving utterances
• recognizing the intention behind an utterance
• displaying or acknowledging this understanding
• keeping track of what has been discussed so far
(common ground due to linguistic co-presence)
• determining when to take a turn
• monitoring and furthering the main purposes or
tasks at hand
• serving other important social needs, such as
face-management
(adapted from Clark &amp; Brennan, 1991)
</listItem>
<bodyText confidence="0.999774828125">
Most of these activities are relatively easy to do
when interaction is face-to-face. However, the
affordances of different media affect the costs of
coordinating these activities. The actual forms of
speech and text corpora are shaped by how
people balance and trade off these costs in the
context of communication.
In a referential communication study, I
compared task-oriented conversations in which
one person either had or didn’t have visual
evidence about the other’s progress (Brennan,
1990). Pairs of people discussed many different
locations on identical maps displayed on
networked computer screens in adjoining
cubicles. The task was for the matcher to get his
car icon parked in the same spot as the car
displayed on only the director’s screen. In one
condition, Visual Evidence, the director could
see the matcher’s car icon and its movements. In
the other, Verbal-Only Evidence, she could not.
In both conditions, they could talk freely.
Language-action transcripts were
produced for a randomly chosen 10% of 480
transcribed interchanges. During each trial, the x
and y coordinates of the matcher&apos;s icon were
recorded and time-stamped, as a moment-by-
moment estimate of where the matcher thought
the target location was. For the sample of 48
trials, I plotted the distance between the
matchers&apos; icon and the target (the director&apos;s icon)
over time, to provide a visible display of how
their beliefs about the target location converged.
Sample time-distance plots are shown in
Figures 1 and 2. Matchers&apos; icons got closer to the
target over time, but not at a steady rate.
Typically, distance diminished relatively steeply
early in the trial, while the matcher interpreted
the director&apos;s initial description and rapidly
moved his icon toward the target location. Many
of the plots then showed a distinct elbow
followed by a nearly horizontal region, meaning
that the matcher then paused or moved away
only slightly before returning to park his car
icon. This suggests that it wasn’t sufficient for
the matcher to develop a reasonable hypothesis
about what the director meant by the description
she presented, but that they also had to ground
their understanding, or exchange sufficient
evidence in order to establish mutual belief. The
region after the elbow appears to correspond to
the acceptance phase proposed by Clark &amp;
Schaefer (1989); the figures show that it was
much shorter when directors had visual evidence
than when they did not. The accompanying
speech transcripts, when synchronized with the
time-distance plots, showed that matchers gave
verbal acknowledgements when directors did not
have visual evidence and withheld them when
directors did have visual evidence. Matchers
made this adjustment to directors even though
the information on the matchers’ own screen
was the same for both conditions, which
alternated after every 10 locations for a total of
80 locations discussed by each pair.
</bodyText>
<figureCaption confidence="0.982397166666667">
Figure 1: Time-Distance Plot of Matcher-Director
Convergence, Without Visual Evidence of the
Matcher’s Progress
Figure 2: Time-Distance Plot of Matcher-Director
Convergence, With Visual Evidence of the Matcher’s
Progress
</figureCaption>
<bodyText confidence="0.999837">
These results document the grounding
process and the time course of how directors’
and matchers’ hypotheses converge. The process
is a flexible one; partners shift the responsibility
to whomever can pay a particular cost most
easily, expending the least collaborative effort
(Clark &amp; Wilkes-Gibbs, 1986).
In another study of how media affect
conversation (Brennan &amp; Ohaeri, 1999; Ohaeri,
1998) we looked at how grounding shapes
conversation held face-to-face vs. via chat
windows in which people sent text messages that
appeared immediately on their partners’ screens.
Three-person groups had to reach a consensus
account of a complex movie clip they had
viewed together. We examined the costs of
serving face-management needs (politeness) and
</bodyText>
<figure confidence="0.9911549">
350
300
250
200
150
100
50
0
0 5 10 15 20 25 30
Time (seconds)
350
300
250
200
150
100
50
0
0 5 10 15 20 25 30
Time (seconds)
</figure>
<bodyText confidence="0.999151590909091">
looked at devices that serve these needs by
giving a partner options or seeking their input.
The devices counted were hedges and questions.
Although both kinds of groups recalled
the events equally well, they produced only half
as many words typing as speaking. There were
much lower rates of hedging (per 100 words) in
the text conversations than face-to-face, but the
same rates of questions. We explained these
findings by appealing to the costs of grounding
over different media: Hedging requires using
additional words, and therefore is more costly in
typed than spoken utterances. Questions, on the
other hand, require only different intonation or
punctuation, and so are equally easy, regardless
of medium. The fact that people used just as
many questions in both kinds of conversations
suggests that people in electronic or remote
groups don’t cease to care about face-
management needs, as some have suggested; it’s
just harder to meet these needs when the
medium makes the primary task more difficult.
</bodyText>
<subsectionHeader confidence="0.777529">
Desiderata for a Dialogue Model
</subsectionHeader>
<bodyText confidence="0.999978547169811">
Findings such as these hold a number of
implications for both computational linguistics
and human-computer interaction. First is a
methodological point: corpus data and dialogue
feature coding are particularly useful when they
include systematic information about the tasks
conversants were engaged in.
Second, there is a large body of
evidence that people accomplish utterance
production and interpretation incrementally,
using information from all available sources in
parallel. If computational language systems are
ever to approach the power, error recovery
ability, and flexibility of human language
processing, then more research needs to be done
using architectures that can support incremental
processing. Architectures should not be based on
assumptions that utterances are complete and
well-formed, and that processing is modular.
A related issue is that timing is critically
important in interactive systems. Many models
of language processing focus on the
propositional content of speech with little
attention to “performance” or “surface” features
such as timing. (Other non-propositional aspects
such as intonation are important as well.)
Computational dialogue systems (both
text and spoken) should include resources for
collaboration. When a new referring expression
is introduced, it could be marked as provisional.
Fillers can be used to display trouble, and
hedges, to invite input. Dialogue models should
track the forms of referring expressions used in a
discourse so far, enabling agents to use the same
terms consistently to refer to the same things.
Because communication media shape
conversations and their emergent corpora, minor
differences in features of a dialogue interface
can have major impact on the form of the
language that is generated, as well as on
coordination costs that language users pay.
Finally, dialogue models should keep a
structured record of jointly achieved
contributions that is updated and revised
incrementally. No agent is omniscient; a
dialogue model represents only one agent&apos;s
estimate of the common ground so far (see Cahn
&amp; Brennan, 1999). There are many open and
interesting questions about how to best structure
the contributions from interacting partners into a
dialogue model, as well as how such a model
can be used to support incremental processes of
generation, interpretation, and repair.
</bodyText>
<sectionHeader confidence="0.994749" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99451175">
This material is based upon work supported by
the National Science Foundation under Grants
No. IRI9402167, IRI9711974, and IRI9980013.
I thank Michael Schober for helpful comments.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989452631579">
Bock, J. K. (1986). Meaning, sound, and syntax:
Lexical priming in sentence production. J. of
Experimental Psychology: Learning, Memory, &amp;
Cognition, 12, 575-586.
Bock, K., &amp; Levelt, W. J. M. (1994). Language
production: Grammatical encoding. In M.A.
Gernsbacher (Ed.), Handbook of psycholinguistics
(pp. 945-984). London: Academic Press.
Bortfeld, H., &amp; Brennan, S. E. (1997). Use and
acquisition of idiomatic expressions in referring by
native and non-native speakers. Discourse
Processes, 23, 119-147.
Bortfeld, H., Leon, S. D., Bloom, J. E., Schober, M.
F., &amp; Brennan, S. E. (2000). Disfluency rates in
spontaneous speech: Effects of age, relationship,
topic, role, and gender. Manuscript under review.
Brennan, S. E. (1990). Seeking and providing
evidence for mutual understanding. Unpublished
doctoral dissertation. Stanford University.
Brennan, S. E. (1996). Lexical entrainment in
spontaneous dialog. Proc. 1996 International
Symposium on Spoken Dialogue (ISSD-96) (pp. 41-
44). Acoustical Society of Japan: Phila., PA.
Brennan, S. E. (to appear). The vocabulary problem in
spoken dialog systems. In S. Luperfoy (Ed.),
Automated Spoken Dialog Systems, Cambridge,
MA: MIT Press.
Brennan, S. E., &amp; Clark, H. H. (1996). Conceptual
pacts and lexical choice in conversation. J. of
Experimental Psychology: Learning, Memory, &amp;
Cognition, 22, 1482-1493.
Brennan, S. E., &amp; Ohaeri, J. O. (1999). Why do
electronic conversations seem less polite? The costs
and benefits of hedging. Proc. Int. Joint Conference
on Work Activities, Coordination, and
Collaboration (WACC ’99) (pp. 227-235). San
Francisco, CA: ACM.
Brennan, S. E., &amp; Schober, M. F. (in press). How
listeners compensate for disfluencies in spontaneous
speech. J. of Memory &amp; Language.
Brennan, S. E., &amp; Williams, M. (1995). The feeling of
another’s knowing: Prosody and filled pauses as
cues to listeners about the metacognitive states of
speakers. J. of Memory &amp; Language, 34, 383-398.
Cahn, J. E., &amp; Brennan, S. E. (1999). A psychological
model of grounding and repair in dialog. Proc. AAAI
Fall Symposium on Psychological Models of
Communication in Collaborative Systems (pp. 25-
33). North Falmouth, MA: AAAI.
Clark, H.H. (1994). Managing problems in speaking.
Speech Communication, 15, 243-250.
Clark, H. H. (1997). Dogmas of understanding.
Discourse Processes, 23, 567-598.
Clark, H. H., &amp; Brennan, S. E. (1991). Grounding in
communication. In L. B. Resnick, J. Levine, &amp; S.
D. Teasley (Eds.), Perspectives on socially shared
cognition (pp. 127-149).
Clark, H.H. &amp; Schaefer, E.F. (1989). Contributing to
discourse. Cognitive Science, 13, 259-294.
Clark, H.H. &amp; Wilkes-Gibbs, D. (1986). Referring as a
collaborative process. Cognition, 22, 1-39.
Core, M. G., &amp; Schubert, L. K. (1999). A model of
speech repairs and other disruptions. Proc. AAAI
Fall Symposium on Psychological Models of
Communication in Collaborative Systems. North
Falmouth, MA: AAAI.
Fox Tree, J.E. (1995). The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. J. of Memory &amp; Language, 34,
709-738.
Goodwin, C. (1981). Conversational Organization:
Interaction between speakers and hearers. New
York: Academic Press.
Hindle, D. (1983). Deterministic parsing of syntactic
non-fluencies. In Proc. of the 21st Annual Meeting,
Association for Computational Linguistics,
Cambridge, MA, pp. 123-128.
Levelt, W. J. M. (1983). Monitoring and self-repair in
speech. Cognition, 14, 41-104.
Levelt, W. (1989). Speaking: From intention to
articulation. Cambridge, MA: MIT Press.
Lickley, R., &amp; Bard, E. (1996). On not recognizing
disfluencies in dialog. Proc. International
Conference on Spoken Language Processing
(ICSLIP ‘96), Philadelphia, 1876-1879.
Nakatani, C. H., &amp; Hirschberg, J. (1994). A corpus-
based study of repair cues in spontaneous speech. J
of the Acoustical Society of America, 95, 1603-1616.
Nooteboom, S. G. (1980). Speaking and unspeaking:
Detection and correction of phonological and lexical
errors in spontaneous speech. In V. A. Fromkin
(Ed.), Errors in linguistic performance. New York:
Academic Press.
Ohaeri, J. O. (1995). Lexical convergence with human
and computer partners: Same cognitive process?
Unpub. Master&apos;s thesis. SUNY, Stony Brook, NY.
Ohaeri, J. O. (1998). Group processes and the
collaborative remembering of stories. Unpublished
doctoral dissertation. SUNY, Stony Brook, NY.
Oviatt, S. (1995). Predicting spoken disfluencies
during human-computer interaction. Computer
Speech and Language, 9, 19-35.
Sacks, H., Schegloff, E., &amp; Jefferson, G. (1974). A
simplest systematics for the organization of turn-
taking in conversation. Language, 50, 696-735.
Schober, M.F. &amp; Clark, H.H. (1989). Understanding
by addressees and overhearers. Cognitive
Psychology, 21, 211-232.
Shriberg, E. (1996). Disfluencies in Switchboard.
Proceedings, International Conference on Spoken
Language Processing, Vol. Addendum, 11-14.
Philadelphia, PA, 3-6 October.
Shriberg, E., Bear, J., &amp; Dowding, J. (1992).
Automatic detection and correction of repairs in
human-computer dialog. In M. Marcus (Ed.), Proc
DARPA Speech and Natural Language Workshop
(pp. 419-424). Morgan Kaufmann.
Shriberg, E.E. &amp; Lickley, R.J. (1993). Intonation of
clause-internal filled pauses. Phonetica, 50, 172-179.
Smith, V., &amp; Clark, H. H. (1993). On the course of
answering questions. J. of Memory and Language,
32, 25-38.
Stellmann, P., &amp; Brennan, S. E. (1993). Flexible
perspective-setting in conversation. Abstracts of the
Psychonomic Society, 34th Annual Meeting (p. 20),
Washington, DC.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard,
K. M., &amp; Sedivy, J. (1995). Integration of visual and
linguistic information in spoken language
comprehension. Science, 268, 1632-1634.
Wilkes-Gibbs, D., &amp; Clark, H.H. (1992). Coordinating
beliefs in conversation. Journal of Memory and
Language, 31, 183-194.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762138">
<title confidence="0.99442">Processes that Shape Conversation and their Implications for Computational Linguistics</title>
<author confidence="0.999993">Susan E Brennan</author>
<affiliation confidence="0.9054845">Department of Psychology State University of New York</affiliation>
<address confidence="0.982336">Stony Brook, NY, US 11794-2500</address>
<email confidence="0.999667">susan.brennan@sunysb.edu</email>
<abstract confidence="0.997221571428571">Experimental studies of interactive language use have shed light on the cognitive and interpersonal processes that shape conversation; corpora are the emergent products of these processes. I will survey studies that focus on under-modelled aspects of interactive language use, including the processing of spontaneous speech and disfluencies; metalinguistic displays such as hedges; interactive processes that affect choices of referring expressions; and how communication media shape conversations. The findings suggest some agendas for computational linguistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Bock</author>
</authors>
<title>Meaning, sound, and syntax: Lexical priming in sentence production.</title>
<date>1986</date>
<journal>J. of Experimental Psychology: Learning, Memory, &amp; Cognition,</journal>
<volume>12</volume>
<pages>575--586</pages>
<contexts>
<context position="10351" citStr="Bock, 1986" startWordPosition="1593" endWordPosition="1594">intonation that helps listeners distinguish them from the rest of the utterance. Fox Tree (1995) found that while previous restarts in an utterance may slow a listener’s monitoring for a particular word, repetitions don’t seem to hurt, and some fillers, such as uh, seem to actually speed monitoring for a subsequent word. What information exists in disfluencies, and how might speakers use it? Speech production processes can be broken into three phases: a message or semantic process, a formulation process in which a syntactic frame is chosen and words are filled in, and an articulation process (Bock, 1986; Bock &amp; Levelt, 1994; Levelt, 1989). Speakers monitor their speech both internally and externally; that is, they can make covert repairs at the point when an internal monitoring loop checks the output of the formulation phase before articulation begins, or overt repairs when a problem is discovered after the articulation phase via the speaker&apos;s external monitor—the point at which listeners also have access to the signal (Levelt, 1989). According to Nooteboom&apos;s (1980) Main Interruption Rule, speakers tend to halt speaking as soon as they detect a problem. Production data from Levelt&apos;s (1983) c</context>
</contexts>
<marker>Bock, 1986</marker>
<rawString>Bock, J. K. (1986). Meaning, sound, and syntax: Lexical priming in sentence production. J. of Experimental Psychology: Learning, Memory, &amp; Cognition, 12, 575-586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bock</author>
<author>W J M Levelt</author>
</authors>
<title>Language production: Grammatical encoding.</title>
<date>1994</date>
<booktitle>In M.A. Gernsbacher (Ed.), Handbook of psycholinguistics</booktitle>
<pages>945--984</pages>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="10372" citStr="Bock &amp; Levelt, 1994" startWordPosition="1595" endWordPosition="1598">hat helps listeners distinguish them from the rest of the utterance. Fox Tree (1995) found that while previous restarts in an utterance may slow a listener’s monitoring for a particular word, repetitions don’t seem to hurt, and some fillers, such as uh, seem to actually speed monitoring for a subsequent word. What information exists in disfluencies, and how might speakers use it? Speech production processes can be broken into three phases: a message or semantic process, a formulation process in which a syntactic frame is chosen and words are filled in, and an articulation process (Bock, 1986; Bock &amp; Levelt, 1994; Levelt, 1989). Speakers monitor their speech both internally and externally; that is, they can make covert repairs at the point when an internal monitoring loop checks the output of the formulation phase before articulation begins, or overt repairs when a problem is discovered after the articulation phase via the speaker&apos;s external monitor—the point at which listeners also have access to the signal (Levelt, 1989). According to Nooteboom&apos;s (1980) Main Interruption Rule, speakers tend to halt speaking as soon as they detect a problem. Production data from Levelt&apos;s (1983) corpus supported this </context>
</contexts>
<marker>Bock, Levelt, 1994</marker>
<rawString>Bock, K., &amp; Levelt, W. J. M. (1994). Language production: Grammatical encoding. In M.A. Gernsbacher (Ed.), Handbook of psycholinguistics (pp. 945-984). London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bortfeld</author>
<author>S E Brennan</author>
</authors>
<title>Use and acquisition of idiomatic expressions in referring by native and non-native speakers.</title>
<date>1997</date>
<booktitle>Discourse Processes,</booktitle>
<volume>23</volume>
<pages>119--147</pages>
<marker>Bortfeld, Brennan, 1997</marker>
<rawString>Bortfeld, H., &amp; Brennan, S. E. (1997). Use and acquisition of idiomatic expressions in referring by native and non-native speakers. Discourse Processes, 23, 119-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bortfeld</author>
<author>S D Leon</author>
<author>J E Bloom</author>
<author>M F Schober</author>
<author>S E Brennan</author>
</authors>
<title>Disfluency rates in spontaneous speech: Effects of age, relationship, topic, role, and gender. Manuscript under review.</title>
<date>2000</date>
<marker>Bortfeld, Leon, Bloom, Schober, Brennan, 2000</marker>
<rawString>Bortfeld, H., Leon, S. D., Bloom, J. E., Schober, M. F., &amp; Brennan, S. E. (2000). Disfluency rates in spontaneous speech: Effects of age, relationship, topic, role, and gender. Manuscript under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
</authors>
<title>Seeking and providing evidence for mutual understanding. Unpublished doctoral dissertation.</title>
<date>1990</date>
<institution>Stanford University.</institution>
<contexts>
<context position="21907" citStr="Brennan, 1990" startWordPosition="3425" endWordPosition="3426">r tasks at hand • serving other important social needs, such as face-management (adapted from Clark &amp; Brennan, 1991) Most of these activities are relatively easy to do when interaction is face-to-face. However, the affordances of different media affect the costs of coordinating these activities. The actual forms of speech and text corpora are shaped by how people balance and trade off these costs in the context of communication. In a referential communication study, I compared task-oriented conversations in which one person either had or didn’t have visual evidence about the other’s progress (Brennan, 1990). Pairs of people discussed many different locations on identical maps displayed on networked computer screens in adjoining cubicles. The task was for the matcher to get his car icon parked in the same spot as the car displayed on only the director’s screen. In one condition, Visual Evidence, the director could see the matcher’s car icon and its movements. In the other, Verbal-Only Evidence, she could not. In both conditions, they could talk freely. Language-action transcripts were produced for a randomly chosen 10% of 480 transcribed interchanges. During each trial, the x and y coordinates of</context>
</contexts>
<marker>Brennan, 1990</marker>
<rawString>Brennan, S. E. (1990). Seeking and providing evidence for mutual understanding. Unpublished doctoral dissertation. Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
</authors>
<title>Lexical entrainment in spontaneous dialog.</title>
<date>1996</date>
<journal>Acoustical Society of Japan:</journal>
<booktitle>Proc. 1996 International Symposium on Spoken Dialogue (ISSD-96)</booktitle>
<pages>41--44</pages>
<publisher>Phila., PA.</publisher>
<contexts>
<context position="19126" citStr="Brennan, 1996" startWordPosition="3012" endWordPosition="3013"> it’s almost like a person kind of in a weird way, and yeah like like a monk praying or something). Hedges dropped out upon repeated referring. We have proposed (Brennan &amp; Clark, 1996) that hedges are devices for signaling a speaker&apos;s commitment to the perspective she is proposing. Hedges serve social needs as well, by inviting counter-proposals from the addressee without risking loss of face due to overt disagreements (Brennan &amp; Ohaeri, 1999). It is worth noting that people&apos;s referring expressions converge not only with those of their human partners, but also with those of computer partners (Brennan, 1996; Ohaeri, 1995). In our text and spoken dialogue Wizardof-Oz studies, when simulated computer partners used deliberately different terms than the ones people first presented to them, people tended to adopt the computers&apos; terms, even though the computers had apparently &amp;quot;understood&amp;quot; the terms people had first produced (Brennan, 1996; Ohaeri, 1995). The impetus toward conceptual entrainment marked by repeated referring expressions appears to be so compelling that native speakers of English will even produce non-idiomatic referring expressions (e.g., the chair in which I shake my body, referring t</context>
</contexts>
<marker>Brennan, 1996</marker>
<rawString>Brennan, S. E. (1996). Lexical entrainment in spontaneous dialog. Proc. 1996 International Symposium on Spoken Dialogue (ISSD-96) (pp. 41-44). Acoustical Society of Japan: Phila., PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S E Brennan</author>
</authors>
<title>(to appear). The vocabulary problem in spoken dialog systems.</title>
<booktitle>In S. Luperfoy (Ed.), Automated Spoken Dialog Systems,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Brennan, </marker>
<rawString>Brennan, S. E. (to appear). The vocabulary problem in spoken dialog systems. In S. Luperfoy (Ed.), Automated Spoken Dialog Systems, Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>H H Clark</author>
</authors>
<title>Conceptual pacts and lexical choice in conversation.</title>
<date>1996</date>
<journal>J. of Experimental Psychology: Learning, Memory, &amp; Cognition,</journal>
<volume>22</volume>
<pages>1482--1493</pages>
<contexts>
<context position="17678" citStr="Brennan and Clark (1996)" startWordPosition="2775" endWordPosition="2778">work, the director and matcher eventually settled on a mutual perspective. When they finished matching the set of 12 picture cards, the cards were shuffled and the task was repeated several more times. In the very next round, the conversation went like this: (2) B: nine is that monk praying A: yup Later on, referring was even more efficient: (3) A: three is the monk B: ok A and B, who switched roles on each round, marked the fact that they had achieved a mutual perspective by reusing the same term, monk, in repeated references to the same object. These references tend to shorten over time. In Brennan and Clark (1996), we showed that once people coordinate a perspective on an object, they tend to continue to use the same terms that mark that shared perspective (e.g., the man’s pennyloafer), even when they could use an even shorter basiclevel term (e.g., the shoe, when the set of objects has changed such that it no longer needs to be distinguished from other shoes in the set). This process of conceptual entrainment appears to be partner-specific—upon repeated referring to the same object but with a new partner, speakers were more likely to revert to the basic level term, due in part to the feedback they rec</context>
<context position="6552" citStr="Brennan &amp; Clark, 1996" startWordPosition="1010" endWordPosition="1013">vers a description in a series of installments, with backchannels from the matcher to confirm them. She seasons her speech with fillers like uh, pauses occasionally, and displays her commitment (or lack thereof) to what she is saying with displays like ah boy this one ah boy and I don’t know what that is. Even though she is the one who knows what the target picture is, it is the matcher who ends up proposing the description that they both end up ratifying: like a monk praying or something. Once the director has ratified this proposal, they have succeeded in establishing a conceptual pact (see Brennan &amp; Clark, 1996). En route, both partners hedged their descriptions liberally, marking them as provisional, pending evidence of acceptance from the other. This example is typical; in fact, 24 pairs of partners who discussed this object ended up synthesizing nearly 24 different but mutually agreed-upon perspectives. Finally, the disfluencies, hedges, and turns would have been distributed quite differently if this conversation had been conducted over a different medium—through instant messaging, or if the partners had had visual contact. Next I will consider the proceses that underlie these aspects of interacti</context>
<context position="18327" citStr="Brennan &amp; Clark, 1996" startWordPosition="2886" endWordPosition="2889">e coordinate a perspective on an object, they tend to continue to use the same terms that mark that shared perspective (e.g., the man’s pennyloafer), even when they could use an even shorter basiclevel term (e.g., the shoe, when the set of objects has changed such that it no longer needs to be distinguished from other shoes in the set). This process of conceptual entrainment appears to be partner-specific—upon repeated referring to the same object but with a new partner, speakers were more likely to revert to the basic level term, due in part to the feedback they received from their partners (Brennan &amp; Clark, 1996). These examples depict the interpersonal processes that lead to conceptual entrainment. The director and matcher used many hedges in their initial proposals and counter-proposals (e.g., it’s almost like a person kind of in a weird way, and yeah like like a monk praying or something). Hedges dropped out upon repeated referring. We have proposed (Brennan &amp; Clark, 1996) that hedges are devices for signaling a speaker&apos;s commitment to the perspective she is proposing. Hedges serve social needs as well, by inviting counter-proposals from the addressee without risking loss of face due to overt disag</context>
</contexts>
<marker>Brennan, Clark, 1996</marker>
<rawString>Brennan, S. E., &amp; Clark, H. H. (1996). Conceptual pacts and lexical choice in conversation. J. of Experimental Psychology: Learning, Memory, &amp; Cognition, 22, 1482-1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>J O Ohaeri</author>
</authors>
<title>Why do electronic conversations seem less polite? The costs and benefits of hedging.</title>
<date>1999</date>
<booktitle>Proc. Int. Joint Conference on Work Activities, Coordination, and Collaboration (WACC ’99)</booktitle>
<pages>227--235</pages>
<publisher>ACM.</publisher>
<location>San Francisco, CA:</location>
<contexts>
<context position="18960" citStr="Brennan &amp; Ohaeri, 1999" startWordPosition="2984" endWordPosition="2987">amples depict the interpersonal processes that lead to conceptual entrainment. The director and matcher used many hedges in their initial proposals and counter-proposals (e.g., it’s almost like a person kind of in a weird way, and yeah like like a monk praying or something). Hedges dropped out upon repeated referring. We have proposed (Brennan &amp; Clark, 1996) that hedges are devices for signaling a speaker&apos;s commitment to the perspective she is proposing. Hedges serve social needs as well, by inviting counter-proposals from the addressee without risking loss of face due to overt disagreements (Brennan &amp; Ohaeri, 1999). It is worth noting that people&apos;s referring expressions converge not only with those of their human partners, but also with those of computer partners (Brennan, 1996; Ohaeri, 1995). In our text and spoken dialogue Wizardof-Oz studies, when simulated computer partners used deliberately different terms than the ones people first presented to them, people tended to adopt the computers&apos; terms, even though the computers had apparently &amp;quot;understood&amp;quot; the terms people had first produced (Brennan, 1996; Ohaeri, 1995). The impetus toward conceptual entrainment marked by repeated referring expressions ap</context>
<context position="24926" citStr="Brennan &amp; Ohaeri, 1999" startWordPosition="3893" endWordPosition="3896">iscussed by each pair. Figure 1: Time-Distance Plot of Matcher-Director Convergence, Without Visual Evidence of the Matcher’s Progress Figure 2: Time-Distance Plot of Matcher-Director Convergence, With Visual Evidence of the Matcher’s Progress These results document the grounding process and the time course of how directors’ and matchers’ hypotheses converge. The process is a flexible one; partners shift the responsibility to whomever can pay a particular cost most easily, expending the least collaborative effort (Clark &amp; Wilkes-Gibbs, 1986). In another study of how media affect conversation (Brennan &amp; Ohaeri, 1999; Ohaeri, 1998) we looked at how grounding shapes conversation held face-to-face vs. via chat windows in which people sent text messages that appeared immediately on their partners’ screens. Three-person groups had to reach a consensus account of a complex movie clip they had viewed together. We examined the costs of serving face-management needs (politeness) and 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) looked at devices that serve these needs by giving a partner options or seeking their input. The devices cou</context>
</contexts>
<marker>Brennan, Ohaeri, 1999</marker>
<rawString>Brennan, S. E., &amp; Ohaeri, J. O. (1999). Why do electronic conversations seem less polite? The costs and benefits of hedging. Proc. Int. Joint Conference on Work Activities, Coordination, and Collaboration (WACC ’99) (pp. 227-235). San Francisco, CA: ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S E Brennan</author>
<author>M F Schober</author>
</authors>
<title>(in press). How listeners compensate for disfluencies in spontaneous speech.</title>
<journal>J. of Memory &amp; Language.</journal>
<marker>Brennan, Schober, </marker>
<rawString>Brennan, S. E., &amp; Schober, M. F. (in press). How listeners compensate for disfluencies in spontaneous speech. J. of Memory &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>M Williams</author>
</authors>
<title>The feeling of another’s knowing: Prosody and filled pauses as cues to listeners about the metacognitive states of speakers.</title>
<date>1995</date>
<journal>J. of Memory &amp; Language,</journal>
<volume>34</volume>
<pages>383--398</pages>
<contexts>
<context position="15451" citStr="Brennan &amp; Williams, 1995" startWordPosition="2424" endWordPosition="2427">peaker does not intend. Listeners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances. People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking (Clark, 1994; Clark, 1997; Clark &amp; Brennan, 1991). Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp; Williams, 1995; Smith &amp; Clark, 1993). In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect. Thus in the following example, A1 displayed a lower FOK than A2: Q: Who founded the American Red Cross? A1: um Florence Nightingale? A2: Clara Barton. Likewise, non-answers (e.g., I don’t know) after a filler or a long latency were produced by speakers who were more likely to recognize the correct answers later on a multiple choice test; those who produced a non-answer immediately did not know the ans</context>
</contexts>
<marker>Brennan, Williams, 1995</marker>
<rawString>Brennan, S. E., &amp; Williams, M. (1995). The feeling of another’s knowing: Prosody and filled pauses as cues to listeners about the metacognitive states of speakers. J. of Memory &amp; Language, 34, 383-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Cahn</author>
<author>S E Brennan</author>
</authors>
<title>A psychological model of grounding and repair in dialog.</title>
<date>1999</date>
<booktitle>Proc. AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems</booktitle>
<pages>25--33</pages>
<publisher>North</publisher>
<location>Falmouth, MA:</location>
<marker>Cahn, Brennan, 1999</marker>
<rawString>Cahn, J. E., &amp; Brennan, S. E. (1999). A psychological model of grounding and repair in dialog. Proc. AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems (pp. 25-33). North Falmouth, MA: AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
</authors>
<title>Managing problems in speaking.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<volume>15</volume>
<pages>243--250</pages>
<contexts>
<context position="1557" citStr="Clark, 1994" startWordPosition="215" endWordPosition="216">ese forces have, until recently, received little attention, having been originally consigned to &amp;quot;performance&amp;quot; by Chomsky, and considered to be of secondary importance by many others. But as anyone who has listened to a tape of herself lecturing surely knows, spoken language is formally quite different from written language. And as those who have transcribed conversation are excruciatingly aware, interactive, spontaneous speech is especially messy and disfluent. This fact is rarely acknowledged by psychological theories of comprehension and production (although see Brennan &amp; Schober, in press; Clark, 1994, 1997; Fox Tree, 1995). In fact, experimental psycholinguists still make up most of their materials, so that much of what we know about sentence processing is based on a sanitized, ideal form of language that no one actually speaks. But the field of computational linguistics has taken an interesting turn: Linguists and computational linguists who formerly used made-up sentences are now using naturally- and experimentally-generated corpora on which to base and test their theories. One of the most exciting developments since the early 1990s has been the focus on corpus data. Organized efforts s</context>
<context position="15213" citStr="Clark, 1994" startWordPosition="2388" endWordPosition="2389">ce interpretation. The form and length of the reparandum and edit interval bear consequences for how quickly a disfluent utterance is processed as well as for whether the listener makes a commitment to an interpretation the speaker does not intend. Listeners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances. People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking (Clark, 1994; Clark, 1997; Clark &amp; Brennan, 1991). Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp; Williams, 1995; Smith &amp; Clark, 1993). In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect. Thus in the following example, A1 displayed a lower FOK than A2: Q: Who founded the American Red Cross? A1: um Florence Nightingale? A2: Clara Barton. Likewise, non-a</context>
</contexts>
<marker>Clark, 1994</marker>
<rawString>Clark, H.H. (1994). Managing problems in speaking. Speech Communication, 15, 243-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
</authors>
<date>1997</date>
<booktitle>Dogmas of understanding. Discourse Processes,</booktitle>
<volume>23</volume>
<pages>567--598</pages>
<contexts>
<context position="15226" citStr="Clark, 1997" startWordPosition="2390" endWordPosition="2391">tion. The form and length of the reparandum and edit interval bear consequences for how quickly a disfluent utterance is processed as well as for whether the listener makes a commitment to an interpretation the speaker does not intend. Listeners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances. People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking (Clark, 1994; Clark, 1997; Clark &amp; Brennan, 1991). Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp; Williams, 1995; Smith &amp; Clark, 1993). In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect. Thus in the following example, A1 displayed a lower FOK than A2: Q: Who founded the American Red Cross? A1: um Florence Nightingale? A2: Clara Barton. Likewise, non-answers (e.g.,</context>
</contexts>
<marker>Clark, 1997</marker>
<rawString>Clark, H. H. (1997). Dogmas of understanding. Discourse Processes, 23, 567-598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>S E Brennan</author>
</authors>
<title>Grounding in communication. In</title>
<date>1991</date>
<pages>127--149</pages>
<contexts>
<context position="15250" citStr="Clark &amp; Brennan, 1991" startWordPosition="2392" endWordPosition="2395">m and length of the reparandum and edit interval bear consequences for how quickly a disfluent utterance is processed as well as for whether the listener makes a commitment to an interpretation the speaker does not intend. Listeners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances. People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking (Clark, 1994; Clark, 1997; Clark &amp; Brennan, 1991). Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp; Williams, 1995; Smith &amp; Clark, 1993). In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect. Thus in the following example, A1 displayed a lower FOK than A2: Q: Who founded the American Red Cross? A1: um Florence Nightingale? A2: Clara Barton. Likewise, non-answers (e.g., I don’t know) after a f</context>
<context position="21409" citStr="Clark &amp; Brennan, 1991" startWordPosition="3347" endWordPosition="3350"> the addressee is likely to understand • producing utterances • recognizing when the addressee does not understand • initiating and managing repairs • determining what inferences to make when there is a delay • receiving utterances • recognizing the intention behind an utterance • displaying or acknowledging this understanding • keeping track of what has been discussed so far (common ground due to linguistic co-presence) • determining when to take a turn • monitoring and furthering the main purposes or tasks at hand • serving other important social needs, such as face-management (adapted from Clark &amp; Brennan, 1991) Most of these activities are relatively easy to do when interaction is face-to-face. However, the affordances of different media affect the costs of coordinating these activities. The actual forms of speech and text corpora are shaped by how people balance and trade off these costs in the context of communication. In a referential communication study, I compared task-oriented conversations in which one person either had or didn’t have visual evidence about the other’s progress (Brennan, 1990). Pairs of people discussed many different locations on identical maps displayed on networked computer</context>
</contexts>
<marker>Clark, Brennan, 1991</marker>
<rawString>Clark, H. H., &amp; Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. Levine, &amp; S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127-149).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>E F Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<volume>13</volume>
<pages>259--294</pages>
<contexts>
<context position="23759" citStr="Clark &amp; Schaefer (1989)" startWordPosition="3721" endWordPosition="3724">ly moved his icon toward the target location. Many of the plots then showed a distinct elbow followed by a nearly horizontal region, meaning that the matcher then paused or moved away only slightly before returning to park his car icon. This suggests that it wasn’t sufficient for the matcher to develop a reasonable hypothesis about what the director meant by the description she presented, but that they also had to ground their understanding, or exchange sufficient evidence in order to establish mutual belief. The region after the elbow appears to correspond to the acceptance phase proposed by Clark &amp; Schaefer (1989); the figures show that it was much shorter when directors had visual evidence than when they did not. The accompanying speech transcripts, when synchronized with the time-distance plots, showed that matchers gave verbal acknowledgements when directors did not have visual evidence and withheld them when directors did have visual evidence. Matchers made this adjustment to directors even though the information on the matchers’ own screen was the same for both conditions, which alternated after every 10 locations for a total of 80 locations discussed by each pair. Figure 1: Time-Distance Plot of </context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Clark, H.H. &amp; Schaefer, E.F. (1989). Contributing to discourse. Cognitive Science, 13, 259-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<volume>22</volume>
<pages>1--39</pages>
<contexts>
<context position="24851" citStr="Clark &amp; Wilkes-Gibbs, 1986" startWordPosition="3881" endWordPosition="3884">ditions, which alternated after every 10 locations for a total of 80 locations discussed by each pair. Figure 1: Time-Distance Plot of Matcher-Director Convergence, Without Visual Evidence of the Matcher’s Progress Figure 2: Time-Distance Plot of Matcher-Director Convergence, With Visual Evidence of the Matcher’s Progress These results document the grounding process and the time course of how directors’ and matchers’ hypotheses converge. The process is a flexible one; partners shift the responsibility to whomever can pay a particular cost most easily, expending the least collaborative effort (Clark &amp; Wilkes-Gibbs, 1986). In another study of how media affect conversation (Brennan &amp; Ohaeri, 1999; Ohaeri, 1998) we looked at how grounding shapes conversation held face-to-face vs. via chat windows in which people sent text messages that appeared immediately on their partners’ screens. Three-person groups had to reach a consensus account of a complex movie clip they had viewed together. We examined the costs of serving face-management needs (politeness) and 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) looked at devices that serve thes</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Clark, H.H. &amp; Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition, 22, 1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Core</author>
<author>L K Schubert</author>
</authors>
<title>A model of speech repairs and other disruptions.</title>
<date>1999</date>
<booktitle>Proc. AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems. North</booktitle>
<publisher>AAAI.</publisher>
<location>Falmouth, MA:</location>
<contexts>
<context position="9175" citStr="Core &amp; Schubert, 1999" startWordPosition="1401" endWordPosition="1404">nrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners di</context>
</contexts>
<marker>Core, Schubert, 1999</marker>
<rawString>Core, M. G., &amp; Schubert, L. K. (1999). A model of speech repairs and other disruptions. Proc. AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems. North Falmouth, MA: AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fox Tree</author>
<author>J E</author>
</authors>
<title>The effects of false starts and repetitions on the processing of subsequent words in spontaneous speech.</title>
<date>1995</date>
<journal>J. of Memory &amp; Language,</journal>
<volume>34</volume>
<pages>709--738</pages>
<marker>Tree, E, 1995</marker>
<rawString>Fox Tree, J.E. (1995). The effects of false starts and repetitions on the processing of subsequent words in spontaneous speech. J. of Memory &amp; Language, 34, 709-738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goodwin</author>
</authors>
<title>Conversational Organization: Interaction between speakers and hearers.</title>
<date>1981</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="2628" citStr="Goodwin, 1981" startWordPosition="374" endWordPosition="375">ase and test their theories. One of the most exciting developments since the early 1990s has been the focus on corpus data. Organized efforts such as LDC and ELRA have assembled large and varied corpora of speech and text, making them widely available to researchers and creators of natural language and speech recognition systems. Finally, Internet usage has generated huge corpora of interactive spontaneous text or &amp;quot;visible conversations&amp;quot; that little resemble edited texts. Of course, ethnographers and sociolinguists who practice conversation analysis (e.g., Sacks, Schegloff, &amp; Jefferson, 1974; Goodwin, 1981) have known for a long time that spontaneous interaction is interesting in its own right, and that although conversation seems messy at first glance, it is actually orderly. Conversation analysts have demonstrated that speakers coordinate with each other such feats as achieving a joint focus of attention, producing closely timed turn exchanges, and finishing each another’s utterances. These demonstrations have been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic language data. But it</context>
</contexts>
<marker>Goodwin, 1981</marker>
<rawString>Goodwin, C. (1981). Conversational Organization: Interaction between speakers and hearers. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Deterministic parsing of syntactic non-fluencies.</title>
<date>1983</date>
<booktitle>In Proc. of the 21st Annual Meeting, Association for Computational Linguistics,</booktitle>
<pages>123--128</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="9189" citStr="Hindle, 1983" startWordPosition="1405" endWordPosition="1406">man speech, then committing to an early and exclusive focus on processing fluent utterances is risky. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners distinguish them</context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, D. (1983). Deterministic parsing of syntactic non-fluencies. In Proc. of the 21st Annual Meeting, Association for Computational Linguistics, Cambridge, MA, pp. 123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Monitoring and self-repair in speech.</title>
<date>1983</date>
<journal>Cognition,</journal>
<volume>14</volume>
<pages>41--104</pages>
<marker>Levelt, 1983</marker>
<rawString>Levelt, W. J. M. (1983). Monitoring and self-repair in speech. Cognition, 14, 41-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
</authors>
<title>Speaking: From intention to articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="10387" citStr="Levelt, 1989" startWordPosition="1599" endWordPosition="1600">istinguish them from the rest of the utterance. Fox Tree (1995) found that while previous restarts in an utterance may slow a listener’s monitoring for a particular word, repetitions don’t seem to hurt, and some fillers, such as uh, seem to actually speed monitoring for a subsequent word. What information exists in disfluencies, and how might speakers use it? Speech production processes can be broken into three phases: a message or semantic process, a formulation process in which a syntactic frame is chosen and words are filled in, and an articulation process (Bock, 1986; Bock &amp; Levelt, 1994; Levelt, 1989). Speakers monitor their speech both internally and externally; that is, they can make covert repairs at the point when an internal monitoring loop checks the output of the formulation phase before articulation begins, or overt repairs when a problem is discovered after the articulation phase via the speaker&apos;s external monitor—the point at which listeners also have access to the signal (Levelt, 1989). According to Nooteboom&apos;s (1980) Main Interruption Rule, speakers tend to halt speaking as soon as they detect a problem. Production data from Levelt&apos;s (1983) corpus supported this rule; speakers </context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>Levelt, W. (1989). Speaking: From intention to articulation. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lickley</author>
<author>E Bard</author>
</authors>
<title>On not recognizing disfluencies in dialog.</title>
<date>1996</date>
<booktitle>Proc. International Conference on Spoken Language Processing (ICSLIP ‘96),</booktitle>
<pages>1876--1879</pages>
<location>Philadelphia,</location>
<contexts>
<context position="9473" citStr="Lickley and Bard (1996)" startWordPosition="1444" endWordPosition="1447">. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners distinguish them from the rest of the utterance. Fox Tree (1995) found that while previous restarts in an utterance may slow a listener’s monitoring for a particular word, repetitions don’t seem to hurt, and some fillers, such as uh, seem to actually speed monitoring for a subsequent word. What info</context>
</contexts>
<marker>Lickley, Bard, 1996</marker>
<rawString>Lickley, R., &amp; Bard, E. (1996). On not recognizing disfluencies in dialog. Proc. International Conference on Spoken Language Processing (ICSLIP ‘96), Philadelphia, 1876-1879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Nakatani</author>
<author>J Hirschberg</author>
</authors>
<title>A corpusbased study of repair cues in spontaneous speech.</title>
<date>1994</date>
<journal>J of the Acoustical Society of America,</journal>
<volume>95</volume>
<pages>1603--1616</pages>
<contexts>
<context position="9218" citStr="Nakatani &amp; Hirschberg, 1994" startWordPosition="1407" endWordPosition="1410">en committing to an early and exclusive focus on processing fluent utterances is risky. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners distinguish them from the rest of the utteran</context>
</contexts>
<marker>Nakatani, Hirschberg, 1994</marker>
<rawString>Nakatani, C. H., &amp; Hirschberg, J. (1994). A corpusbased study of repair cues in spontaneous speech. J of the Acoustical Society of America, 95, 1603-1616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Nooteboom</author>
</authors>
<title>Speaking and unspeaking: Detection and correction of phonological and lexical errors in spontaneous speech. In</title>
<date>1980</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>Nooteboom, 1980</marker>
<rawString>Nooteboom, S. G. (1980). Speaking and unspeaking: Detection and correction of phonological and lexical errors in spontaneous speech. In V. A. Fromkin (Ed.), Errors in linguistic performance. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J O Ohaeri</author>
</authors>
<title>Lexical convergence with human and computer partners: Same cognitive process? Unpub. Master&apos;s thesis. SUNY,</title>
<date>1995</date>
<location>Stony Brook, NY.</location>
<contexts>
<context position="19141" citStr="Ohaeri, 1995" startWordPosition="3014" endWordPosition="3015">ke a person kind of in a weird way, and yeah like like a monk praying or something). Hedges dropped out upon repeated referring. We have proposed (Brennan &amp; Clark, 1996) that hedges are devices for signaling a speaker&apos;s commitment to the perspective she is proposing. Hedges serve social needs as well, by inviting counter-proposals from the addressee without risking loss of face due to overt disagreements (Brennan &amp; Ohaeri, 1999). It is worth noting that people&apos;s referring expressions converge not only with those of their human partners, but also with those of computer partners (Brennan, 1996; Ohaeri, 1995). In our text and spoken dialogue Wizardof-Oz studies, when simulated computer partners used deliberately different terms than the ones people first presented to them, people tended to adopt the computers&apos; terms, even though the computers had apparently &amp;quot;understood&amp;quot; the terms people had first produced (Brennan, 1996; Ohaeri, 1995). The impetus toward conceptual entrainment marked by repeated referring expressions appears to be so compelling that native speakers of English will even produce non-idiomatic referring expressions (e.g., the chair in which I shake my body, referring to a rocking cha</context>
</contexts>
<marker>Ohaeri, 1995</marker>
<rawString>Ohaeri, J. O. (1995). Lexical convergence with human and computer partners: Same cognitive process? Unpub. Master&apos;s thesis. SUNY, Stony Brook, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J O Ohaeri</author>
</authors>
<title>Group processes and the collaborative remembering of stories. Unpublished doctoral dissertation. SUNY,</title>
<date>1998</date>
<location>Stony Brook, NY.</location>
<contexts>
<context position="24941" citStr="Ohaeri, 1998" startWordPosition="3897" endWordPosition="3898">igure 1: Time-Distance Plot of Matcher-Director Convergence, Without Visual Evidence of the Matcher’s Progress Figure 2: Time-Distance Plot of Matcher-Director Convergence, With Visual Evidence of the Matcher’s Progress These results document the grounding process and the time course of how directors’ and matchers’ hypotheses converge. The process is a flexible one; partners shift the responsibility to whomever can pay a particular cost most easily, expending the least collaborative effort (Clark &amp; Wilkes-Gibbs, 1986). In another study of how media affect conversation (Brennan &amp; Ohaeri, 1999; Ohaeri, 1998) we looked at how grounding shapes conversation held face-to-face vs. via chat windows in which people sent text messages that appeared immediately on their partners’ screens. Three-person groups had to reach a consensus account of a complex movie clip they had viewed together. We examined the costs of serving face-management needs (politeness) and 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) 350 300 250 200 150 100 50 0 0 5 10 15 20 25 30 Time (seconds) looked at devices that serve these needs by giving a partner options or seeking their input. The devices counted were hedge</context>
</contexts>
<marker>Ohaeri, 1998</marker>
<rawString>Ohaeri, J. O. (1998). Group processes and the collaborative remembering of stories. Unpublished doctoral dissertation. SUNY, Stony Brook, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
</authors>
<title>Predicting spoken disfluencies during human-computer interaction.</title>
<date>1995</date>
<journal>Computer Speech and Language,</journal>
<volume>9</volume>
<contexts>
<context position="7849" citStr="Oviatt, 1995" startWordPosition="1201" endWordPosition="1202"> implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren&apos;t disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing. The first assumption is clearly false; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses. The rate is lower for speech to machines (Oviatt, 1995; Shriberg, 1996), due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative. The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully. The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could othe</context>
</contexts>
<marker>Oviatt, 1995</marker>
<rawString>Oviatt, S. (1995). Predicting spoken disfluencies during human-computer interaction. Computer Speech and Language, 9, 19-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turntaking in conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<pages>696--735</pages>
<contexts>
<context position="2612" citStr="Sacks, Schegloff, &amp; Jefferson, 1974" startWordPosition="369" endWordPosition="373">tally-generated corpora on which to base and test their theories. One of the most exciting developments since the early 1990s has been the focus on corpus data. Organized efforts such as LDC and ELRA have assembled large and varied corpora of speech and text, making them widely available to researchers and creators of natural language and speech recognition systems. Finally, Internet usage has generated huge corpora of interactive spontaneous text or &amp;quot;visible conversations&amp;quot; that little resemble edited texts. Of course, ethnographers and sociolinguists who practice conversation analysis (e.g., Sacks, Schegloff, &amp; Jefferson, 1974; Goodwin, 1981) have known for a long time that spontaneous interaction is interesting in its own right, and that although conversation seems messy at first glance, it is actually orderly. Conversation analysts have demonstrated that speakers coordinate with each other such feats as achieving a joint focus of attention, producing closely timed turn exchanges, and finishing each another’s utterances. These demonstrations have been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic langu</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Sacks, H., Schegloff, E., &amp; Jefferson, G. (1974). A simplest systematics for the organization of turntaking in conversation. Language, 50, 696-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Schober</author>
<author>H H Clark</author>
</authors>
<title>Understanding by addressees and overhearers.</title>
<date>1989</date>
<journal>Cognitive Psychology,</journal>
<volume>21</volume>
<pages>211--232</pages>
<contexts>
<context position="3637" citStr="Schober &amp; Clark, 1989" startWordPosition="518" endWordPosition="521">These demonstrations have been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic language data. But it is important to keep in mind that a corpus is, after all, only an artifact—a product that emerges from the processes that occur between and within speakers and addressees. Researchers who analyze the textual records of conversation are only overhearers, and there is ample evidence that overhearers experience a conversation quite differently from addressees and from side participants (Schober &amp; Clark, 1989; Wilkes-Gibbs &amp; Clark, 1992). With a corpus alone, there is no independent evidence of what people actually intend or understand at different points in a conversation, or why they make the choices they do. Conversation experiments that provide partners with a task to do have much to offer, such as independent measures of communicative success as well as evidence of precisely when one partner is confused or has reached a hypothesis about the other’s beliefs or intentions. Task-oriented corpora in combination with information about how they were generated are important for discourse studies. We</context>
</contexts>
<marker>Schober, Clark, 1989</marker>
<rawString>Schober, M.F. &amp; Clark, H.H. (1989). Understanding by addressees and overhearers. Cognitive Psychology, 21, 211-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<date>1996</date>
<booktitle>Disfluencies in Switchboard. Proceedings, International Conference on Spoken Language Processing,</booktitle>
<volume>Vol.</volume>
<pages>11--14</pages>
<publisher>Addendum,</publisher>
<location>Philadelphia, PA,</location>
<contexts>
<context position="7866" citStr="Shriberg, 1996" startWordPosition="1203" endWordPosition="1204">mptions of psychological and computational theories that ignore disfluencies must be either that people aren&apos;t disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing. The first assumption is clearly false; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses. The rate is lower for speech to machines (Oviatt, 1995; Shriberg, 1996), due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative. The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully. The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused </context>
</contexts>
<marker>Shriberg, 1996</marker>
<rawString>Shriberg, E. (1996). Disfluencies in Switchboard. Proceedings, International Conference on Spoken Language Processing, Vol. Addendum, 11-14. Philadelphia, PA, 3-6 October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>J Bear</author>
<author>J Dowding</author>
</authors>
<title>Automatic detection and correction of repairs in human-computer dialog. In</title>
<date>1992</date>
<booktitle>Proc DARPA Speech and Natural Language Workshop</booktitle>
<pages>419--424</pages>
<editor>M. Marcus (Ed.),</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9251" citStr="Shriberg, Bear, &amp; Dowding, 1992" startWordPosition="1411" endWordPosition="1415"> exclusive focus on processing fluent utterances is risky. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners distinguish them from the rest of the utterance. Fox Tree (1995) found that wh</context>
</contexts>
<marker>Shriberg, Bear, Dowding, 1992</marker>
<rawString>Shriberg, E., Bear, J., &amp; Dowding, J. (1992). Automatic detection and correction of repairs in human-computer dialog. In M. Marcus (Ed.), Proc DARPA Speech and Natural Language Workshop (pp. 419-424). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Shriberg</author>
<author>R J Lickley</author>
</authors>
<title>Intonation of clause-internal filled pauses.</title>
<date>1993</date>
<journal>Phonetica,</journal>
<volume>50</volume>
<pages>172--179</pages>
<contexts>
<context position="9669" citStr="Shriberg and Lickley (1993)" startWordPosition="1479" endWordPosition="1482">l-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley and Bard (1996) found evidence that listeners may be relatively deaf to the words in a reparandum (the part that would need to be excised in order for the utterance to be fluent), and Shriberg and Lickley (1993) found that fillers such as um or uh may be produced with a distinctive intonation that helps listeners distinguish them from the rest of the utterance. Fox Tree (1995) found that while previous restarts in an utterance may slow a listener’s monitoring for a particular word, repetitions don’t seem to hurt, and some fillers, such as uh, seem to actually speed monitoring for a subsequent word. What information exists in disfluencies, and how might speakers use it? Speech production processes can be broken into three phases: a message or semantic process, a formulation process in which a syntacti</context>
</contexts>
<marker>Shriberg, Lickley, 1993</marker>
<rawString>Shriberg, E.E. &amp; Lickley, R.J. (1993). Intonation of clause-internal filled pauses. Phonetica, 50, 172-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Smith</author>
<author>H H Clark</author>
</authors>
<title>On the course of answering questions.</title>
<date>1993</date>
<journal>J. of Memory and Language,</journal>
<volume>32</volume>
<pages>25--38</pages>
<contexts>
<context position="15473" citStr="Smith &amp; Clark, 1993" startWordPosition="2428" endWordPosition="2431">steners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances. People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking (Clark, 1994; Clark, 1997; Clark &amp; Brennan, 1991). Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp; Williams, 1995; Smith &amp; Clark, 1993). In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect. Thus in the following example, A1 displayed a lower FOK than A2: Q: Who founded the American Red Cross? A1: um Florence Nightingale? A2: Clara Barton. Likewise, non-answers (e.g., I don’t know) after a filler or a long latency were produced by speakers who were more likely to recognize the correct answers later on a multiple choice test; those who produced a non-answer immediately did not know the answers. Not only do spea</context>
</contexts>
<marker>Smith, Clark, 1993</marker>
<rawString>Smith, V., &amp; Clark, H. H. (1993). On the course of answering questions. J. of Memory and Language, 32, 25-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stellmann</author>
<author>S E Brennan</author>
</authors>
<title>Flexible perspective-setting in conversation.</title>
<date>1993</date>
<booktitle>Abstracts of the Psychonomic Society, 34th Annual Meeting (p. 20),</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="5729" citStr="Stellmann &amp; Brennan, 1993" startWordPosition="871" endWordPosition="874">ial communication. A director and a matcher who could not see each another were trying to get identical sets of picture cards lined up in the same order. (1) D:ah boy this one ah boy all right it looks kinda likeon the right top there’s a square that looks diagonal M: uh huh D: and you have sort of another like rectangle shape, thelike a triangle, angled, and on the bottom it’s uh I don’t know what that is, glass shaped M: all right I think I got it D: it’s almost like a person kind of in a weird way M: yeah like like a monk praying or something D: right yeah good great M: all right I got it (Stellmann &amp; Brennan, 1993) Several things are apparent from this exchange. First, it contains several disfluencies or interruptions in fluent speech. The director restarts her first turn twice and her second turn once. She delivers a description in a series of installments, with backchannels from the matcher to confirm them. She seasons her speech with fillers like uh, pauses occasionally, and displays her commitment (or lack thereof) to what she is saying with displays like ah boy this one ah boy and I don’t know what that is. Even though she is the one who knows what the target picture is, it is the matcher who ends </context>
</contexts>
<marker>Stellmann, Brennan, 1993</marker>
<rawString>Stellmann, P., &amp; Brennan, S. E. (1993). Flexible perspective-setting in conversation. Abstracts of the Psychonomic Society, 34th Annual Meeting (p. 20), Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tanenhaus</author>
<author>M J Spivey-Knowlton</author>
<author>K M Eberhard</author>
<author>J Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<volume>268</volume>
<pages>1632--1634</pages>
<contexts>
<context position="8857" citStr="Tanenhaus et al. 1995" startWordPosition="1354" endWordPosition="1357">eech processors, and plan their utterances to machines more carefully. The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself. As for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky. In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995). This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance. Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp; Schubert, 1999; Hindle, 1983; Nakatani &amp; Hirschberg, 1994; Shriberg, Bear, &amp; Dowding, 1992). The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech. Lickley</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., &amp; Sedivy, J. (1995). Integration of visual and linguistic information in spoken language comprehension. Science, 268, 1632-1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wilkes-Gibbs</author>
<author>H H Clark</author>
</authors>
<title>Coordinating beliefs in conversation.</title>
<date>1992</date>
<journal>Journal of Memory and Language,</journal>
<volume>31</volume>
<pages>183--194</pages>
<contexts>
<context position="3666" citStr="Wilkes-Gibbs &amp; Clark, 1992" startWordPosition="522" endWordPosition="525">ve been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic language data. But it is important to keep in mind that a corpus is, after all, only an artifact—a product that emerges from the processes that occur between and within speakers and addressees. Researchers who analyze the textual records of conversation are only overhearers, and there is ample evidence that overhearers experience a conversation quite differently from addressees and from side participants (Schober &amp; Clark, 1989; Wilkes-Gibbs &amp; Clark, 1992). With a corpus alone, there is no independent evidence of what people actually intend or understand at different points in a conversation, or why they make the choices they do. Conversation experiments that provide partners with a task to do have much to offer, such as independent measures of communicative success as well as evidence of precisely when one partner is confused or has reached a hypothesis about the other’s beliefs or intentions. Task-oriented corpora in combination with information about how they were generated are important for discourse studies. We still don&apos;t know nearly enou</context>
</contexts>
<marker>Wilkes-Gibbs, Clark, 1992</marker>
<rawString>Wilkes-Gibbs, D., &amp; Clark, H.H. (1992). Coordinating beliefs in conversation. Journal of Memory and Language, 31, 183-194.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>